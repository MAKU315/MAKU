{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data mnist\n"
    " # OReilly.Hands－On.Machine.Learning.with.Scikit－Learn.and.TensorFlow book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs =10\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs),name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None),name=\"y\")\n",
    "\n",
    "def neuron_layer(X,n_neurons, name, activation = None ):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2/np.sqrt(n_inputs)\n",
    "        init= tf.truncated_normal((n_inputs, n_neurons),stddev = stddev)\n",
    "        W = tf.Variable(init, name=\"weight\" )\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bais\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None :\n",
    "            return activation(X)\n",
    "        else:\n",
    "            return Z\n",
    "        \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X,n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1,n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits= logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y , 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train accuracy: 0.9 test accuracy 0.9104\n",
      "1 train accuracy: 0.92 test accuracy 0.9035\n",
      "2 train accuracy: 0.92 test accuracy 0.9191\n",
      "3 train accuracy: 0.94 test accuracy 0.9128\n",
      "4 train accuracy: 0.92 test accuracy 0.9115\n",
      "5 train accuracy: 0.92 test accuracy 0.911\n",
      "6 train accuracy: 0.92 test accuracy 0.9175\n",
      "7 train accuracy: 0.9 test accuracy 0.9169\n",
      "8 train accuracy: 0.96 test accuracy 0.9168\n",
      "9 train accuracy: 0.9 test accuracy 0.9157\n",
      "10 train accuracy: 0.96 test accuracy 0.915\n",
      "11 train accuracy: 0.94 test accuracy 0.9097\n",
      "12 train accuracy: 0.94 test accuracy 0.9138\n",
      "13 train accuracy: 0.96 test accuracy 0.9115\n",
      "14 train accuracy: 0.92 test accuracy 0.918\n",
      "15 train accuracy: 0.98 test accuracy 0.9185\n",
      "16 train accuracy: 1.0 test accuracy 0.9084\n",
      "17 train accuracy: 0.96 test accuracy 0.9102\n",
      "18 train accuracy: 0.92 test accuracy 0.9112\n",
      "19 train accuracy: 0.98 test accuracy 0.9138\n",
      "20 train accuracy: 0.88 test accuracy 0.9105\n",
      "21 train accuracy: 0.88 test accuracy 0.9114\n",
      "22 train accuracy: 0.98 test accuracy 0.909\n",
      "23 train accuracy: 0.8 test accuracy 0.9116\n",
      "24 train accuracy: 0.9 test accuracy 0.9141\n",
      "25 train accuracy: 0.92 test accuracy 0.91\n",
      "26 train accuracy: 0.94 test accuracy 0.9114\n",
      "27 train accuracy: 0.94 test accuracy 0.9142\n",
      "28 train accuracy: 0.98 test accuracy 0.9096\n",
      "29 train accuracy: 1.0 test accuracy 0.9188\n",
      "30 train accuracy: 0.9 test accuracy 0.9075\n",
      "31 train accuracy: 0.94 test accuracy 0.9088\n",
      "32 train accuracy: 0.94 test accuracy 0.9103\n",
      "33 train accuracy: 0.94 test accuracy 0.9075\n",
      "34 train accuracy: 0.94 test accuracy 0.9051\n",
      "35 train accuracy: 0.98 test accuracy 0.9106\n",
      "36 train accuracy: 0.94 test accuracy 0.9114\n",
      "37 train accuracy: 1.0 test accuracy 0.9071\n",
      "38 train accuracy: 0.92 test accuracy 0.9174\n",
      "39 train accuracy: 0.98 test accuracy 0.9143\n",
      "40 train accuracy: 0.92 test accuracy 0.9143\n",
      "41 train accuracy: 0.92 test accuracy 0.9152\n",
      "42 train accuracy: 0.98 test accuracy 0.9085\n",
      "43 train accuracy: 0.92 test accuracy 0.9108\n",
      "44 train accuracy: 0.92 test accuracy 0.9127\n",
      "45 train accuracy: 0.9 test accuracy 0.9181\n",
      "46 train accuracy: 0.94 test accuracy 0.9132\n",
      "47 train accuracy: 0.92 test accuracy 0.9053\n",
      "48 train accuracy: 0.9 test accuracy 0.9069\n",
      "49 train accuracy: 0.94 test accuracy 0.9031\n",
      "50 train accuracy: 0.98 test accuracy 0.9145\n",
      "51 train accuracy: 0.94 test accuracy 0.9119\n",
      "52 train accuracy: 0.98 test accuracy 0.9023\n",
      "53 train accuracy: 0.92 test accuracy 0.9156\n",
      "54 train accuracy: 0.86 test accuracy 0.9107\n",
      "55 train accuracy: 0.9 test accuracy 0.9156\n",
      "56 train accuracy: 0.94 test accuracy 0.91\n",
      "57 train accuracy: 0.86 test accuracy 0.9067\n",
      "58 train accuracy: 0.98 test accuracy 0.9061\n",
      "59 train accuracy: 0.94 test accuracy 0.9079\n",
      "60 train accuracy: 0.9 test accuracy 0.9098\n",
      "61 train accuracy: 0.92 test accuracy 0.9175\n",
      "62 train accuracy: 0.92 test accuracy 0.9172\n",
      "63 train accuracy: 0.88 test accuracy 0.9128\n",
      "64 train accuracy: 0.94 test accuracy 0.9106\n",
      "65 train accuracy: 0.92 test accuracy 0.9021\n",
      "66 train accuracy: 0.96 test accuracy 0.9115\n",
      "67 train accuracy: 0.92 test accuracy 0.9109\n",
      "68 train accuracy: 0.98 test accuracy 0.9158\n",
      "69 train accuracy: 0.92 test accuracy 0.9072\n",
      "70 train accuracy: 0.98 test accuracy 0.9104\n",
      "71 train accuracy: 0.98 test accuracy 0.907\n",
      "72 train accuracy: 0.92 test accuracy 0.9129\n",
      "73 train accuracy: 0.96 test accuracy 0.9097\n",
      "74 train accuracy: 0.86 test accuracy 0.9045\n",
      "75 train accuracy: 0.92 test accuracy 0.9153\n",
      "76 train accuracy: 0.88 test accuracy 0.9126\n",
      "77 train accuracy: 0.94 test accuracy 0.9105\n",
      "78 train accuracy: 0.86 test accuracy 0.9143\n",
      "79 train accuracy: 0.94 test accuracy 0.9123\n",
      "80 train accuracy: 0.94 test accuracy 0.9133\n",
      "81 train accuracy: 0.9 test accuracy 0.9166\n",
      "82 train accuracy: 0.92 test accuracy 0.9138\n",
      "83 train accuracy: 0.92 test accuracy 0.9104\n",
      "84 train accuracy: 0.9 test accuracy 0.9107\n",
      "85 train accuracy: 0.94 test accuracy 0.9118\n",
      "86 train accuracy: 0.98 test accuracy 0.9073\n",
      "87 train accuracy: 0.92 test accuracy 0.9059\n",
      "88 train accuracy: 0.88 test accuracy 0.9008\n",
      "89 train accuracy: 0.96 test accuracy 0.9185\n",
      "90 train accuracy: 0.9 test accuracy 0.9056\n",
      "91 train accuracy: 0.9 test accuracy 0.9079\n",
      "92 train accuracy: 0.94 test accuracy 0.9111\n",
      "93 train accuracy: 0.92 test accuracy 0.9106\n",
      "94 train accuracy: 0.9 test accuracy 0.9124\n",
      "95 train accuracy: 0.96 test accuracy 0.916\n",
      "96 train accuracy: 0.9 test accuracy 0.9079\n",
      "97 train accuracy: 1.0 test accuracy 0.9078\n",
      "98 train accuracy: 0.94 test accuracy 0.9154\n",
      "99 train accuracy: 0.96 test accuracy 0.9151\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for interation in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict ={ X : X_batch, y : y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict= { X : X_batch, y : y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"train accuracy:\", acc_train, \"test accuracy\", acc_test)\n",
    "    \n",
    "    sava_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 2 4]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    X_new_scaled = mnist.test.images[:20]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1) # softmax를 사용하지 않고 logit 갑중에서 가장 값을 사용했다.\n",
    "    # 일종의 트릭 \n",
    "    \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Neural Network Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search는 좋지 않을 수 있다.\n",
    "### randomized search 나 Oscar tool을 사용\n",
    "### 합리적인 hyper parameter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "적은 layer 보다 깊은 layer가 parameter 의 효울이 더 높다\n",
    "shallow < deeper \n",
    "\n",
    "더 빨리 good solution, and 다른 데이터셋에 활용 가능성 증가\n",
    "\n",
    "저 레벨의 학습을 유사한 다를 학습에 사용할 수 있다.\n",
    "\n",
    "예 ) 선에 대한 학습-> 삼각형, 원 -> 얼굴\n",
    "\n",
    "이전 학습 값 -> 헤어스타일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Number of Neurons per Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확한.. 방법은 없다\n",
    "\n",
    "간단한 방법은 일단 매우 복잡한 모델 -> early stop, regulariztion technique. drop out\n",
    "\n",
    "stretch pants \"신축성 바지\"\n",
    "딱 맞는 모델을 찾지 말고 크게 만들어서 줄여라"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuLU 함수를 많이 사용 : 빠르고 not Stuck on plateaus 정체 현상ㅇ에 빠지지 않음\n",
    "not saturate as opposd to the logistic ftn or hyperbolic tangent fun\n",
    "\n",
    "\n",
    "Output layer : softmax activation - classfication( mutally exclusive ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch11 Training Deep Neural Nets\n",
    "\n",
    "\n",
    "1. tricky vanishing gradients problem ( vanishing gradients or exploding problem) \n",
    "        - 저 레벨 layer의 학습을 힘들게 함\n",
    "\n",
    "2. large network -> training extremely slow\n",
    "\n",
    "3. millions of parameters -> risk overfitting the training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서로 다른 계층은 서로 다른 속도를 가지고 있다\n",
    "Exploding gradients problem은 RNN에서 많이 발생하는 문제 이다.\n",
    "\n",
    "저레벨 레이어는 거의 weight가 업데이트 되지 않는다. \n",
    "\n",
    "- 이 문제에 대해서 2010년 경에 많은 이해를 하게 되었다\n",
    "\n",
    "각 레이어의 아웃풋의 분산은  인풋의 분산보다 더 커진다.\n",
    "\n",
    "상위 레이어에서 활성함수가 saturate 할 때 까지 각 레이어의 분산은 계속 증가한다.\n",
    "\n",
    "- 로지스틱 함수는 평균이 0이 아니라 0.5 인 것도 문제를 더 악화 시킨다.\n",
    "-> tahn 함수가 더 잘 작동한다.\n",
    "\n",
    "\n",
    "로지스틱 함수.. saturating -> 하위 레이어는 더더 작아져 아무것도 학습하지 못한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier and He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input 과 output의 분산을 동일하게 설정을 제안\n",
    "\n",
    "http://deepdish.io/2015/02/24/network-initialization/\n",
    "\n",
    "initialization 과 관련된 사이트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기화를 해야하는가? \n",
    "ReLU로 해결되었는가?\n",
    "\n",
    "RNN 같은 경우 LSTM을 활용.. memory cell 을 이용하여\n",
    "\n",
    "분산이 균일해야 모든 층을 고르게 학습할수 있다.\n",
    "-> var(output)/var(input) = n* var(W) 이다.\n",
    "\n",
    "n*var(W) =1 이면 좋을것이다.\n",
    "\n",
    ".. 이 논문의 핵심"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
